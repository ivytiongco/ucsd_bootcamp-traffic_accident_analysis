{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_dir = '../data/raw/'\n",
    "interim_data_dir = '../data/interim/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "url= \"https://crashviewer.nhtsa.dot.gov/CrashAPI\"\n",
    "#/crashes/GetCrashesByLocation?fromCaseYear=2014&toCaseYear=2015&state=1&county=1&format=json\n",
    "\n",
    "fromCaseYear = \"2010\"\n",
    "toCaseYear = \"2020\"\n",
    "state = \"6\"\n",
    "qurl = f\"{url}/crashes/GetCrashesByLocation?fromCaseYear={fromCaseYear}&toCaseYear={toCaseYear}&state={state}&county=73&format=json\"\n",
    "\n",
    "cali = requests.get(qurl).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cali_df = pd.DataFrame(cali['Results'][0]).groupby('ST_CASE', as_index=False).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2015\n",
    "st_case = 60022\n",
    "\n",
    "qurl = f\"{url}/crashes/GetCaseDetails?stateCase={st_case}&caseYear={year}&state=6&format=json\"\n",
    "data = requests.get(qurl).json()\n",
    "case = data['Results'][0][0]['CrashResultSet']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ARR_HOUR', 'ARR_HOURNAME', 'ARR_MIN', 'ARR_MINNAME', 'CEvents', 'CF1', 'CF1NAME', 'CF2', 'CF2NAME', 'CF3', 'CF3NAME', 'CITY', 'CITYNAME', 'COUNTY', 'COUNTYNAME', 'CaseYear', 'DAY', 'DAY_WEEK', 'DAY_WEEKNAME', 'DRUNK_DR', 'FATALS', 'FUNC_SYS', 'FUNC_SYSNAME', 'HARM_EV', 'HARM_EVNAME', 'HOSP_HR', 'HOSP_HRNAME', 'HOSP_MN', 'HOSP_MNNAME', 'HOUR', 'HOURNAME', 'LATITUDE', 'LATITUDENAME', 'LGT_COND', 'LGT_CONDNAME', 'LONGITUD', 'LONGITUDNAME', 'MAN_COLL', 'MAN_COLLNAME', 'MILEPT', 'MILEPTNAME', 'MINUTE', 'MINUTENAME', 'MONTH', 'MonthName', 'NHS', 'NHSNAME', 'NOT_HOUR', 'NOT_HOURNAME', 'NOT_MIN', 'NOT_MINNAME', 'NPersons', 'NmCrashes', 'NmImpairs', 'NmPriors', 'PEDS', 'PERMVIT', 'PERNOTMVIT', 'PERSONS', 'PVH_INVL', 'ParkWorks', 'PbTypes', 'RAIL', 'RAILNAME', 'RD_OWNER', 'RD_OWNERNAME', 'RELJCT1', 'RELJCT1NAME', 'RELJCT2', 'RELJCT2NAME', 'REL_ROAD', 'REL_ROADNAME', 'ROAD_FNC', 'ROAD_FNCNAME', 'ROUTE', 'ROUTENAME', 'RUR_URB', 'RUR_URBNAME', 'SCH_BUS', 'SCH_BUSNAME', 'SP_JUR', 'SP_JURNAME', 'STATENAME', 'ST_CASE', 'SafetyEQs', 'State', 'TWAY_ID', 'TWAY_ID2', 'TYP_INT', 'TYP_INTNAME', 'VE_FORMS', 'VE_TOTAL', 'Vehicles', 'WEATHER', 'WEATHER1', 'WEATHER1NAME', 'WEATHER2', 'WEATHER2NAME', 'WEATHERNAME', 'WRK_ZONE', 'WRK_ZONENAME', 'YEAR'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-117.063127780'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case['LONGITUD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_people(v):\n",
    "    for p in v['Persons']:\n",
    "        yield {\n",
    "            'Speed Limit Exceeded': v['SPEEDRELNAME'],\n",
    "            'Speed limit': v['TRAV_SP'],\n",
    "            'Vin Number': v['VINNAME'],\n",
    "            'Traveled Speed Veh': v['VSPD_LIM'],\n",
    "            'Make': v['MAKENAME'],\n",
    "            'Make/Model': v['MAK_MODNAME'],\n",
    "            'Model': v['MODELNAME'],\n",
    "            'Type of Vehicle': v['BODY_TYPNAME'],\n",
    "            \"Driver ZIP Code\": v['DR_ZIP'],\n",
    "            **person_dict(p),\n",
    "        }\n",
    "\n",
    "def person_dict(p):\n",
    "    return {\n",
    "        \"Age\": p['AGE'],\n",
    "        \"Age Name\": p['AGENAME'],\n",
    "        \"County\": p['COUNTYNAME'],\n",
    "        \"Death Day of Month\": p['DEATH_DANAME'],\n",
    "        \"DOA Name\": p['DOANAME'],\n",
    "        # injury sev\n",
    "        \"Injury Severity Name\": p['INJ_SEVNAME'],\n",
    "        \"Race\": p['RACENAME'],\n",
    "        \"Road Type\": p[\"ROAD_FNCNAME\"],\n",
    "        \"Sex\": p[\"SEXNAME\"],\n",
    "        \"Make\": p[\"MAKENAME\"],\n",
    "    }\n",
    "\n",
    "        \n",
    "def get_people(case):\n",
    "\n",
    "    hour = case['HOUR']\n",
    "    minute = case['MINUTE']\n",
    "    time = f\"{hour}:{minute}\"\n",
    "        \n",
    "    accident_info = {\n",
    "        'Lng': case['LONGITUD'],\n",
    "        'Lat': case['LATITUDE'],\n",
    "        'Case Number': case['ST_CASE'],\n",
    "        \"Description of Veh Coll\": case['CF2NAME'], \n",
    "        \"Day of Week\": case['DAY_WEEKNAME'],\n",
    "        \"Drunk Driver\": case['DRUNK_DR'],\n",
    "        \"Year\": case['CaseYear'],\n",
    "        \"Month\": case['MonthName'],\n",
    "        \"Hour\": hour,\n",
    "        \"Time of Accident\": time,\n",
    "    }\n",
    "\n",
    "    vehicles = case['Vehicles']\n",
    "    pk = 'NPersons'\n",
    "    pedestrians = [] if (pk not in case.keys() or case[pk] is None) else case[pk] \n",
    "    \n",
    "    people = [{**accident_info, **p} for v in vehicles for p in extract_people(v)] \\\n",
    "        + [{**accident_info, **person_dict(p)} for p in pedestrians]\n",
    "    return pd.DataFrame(people)\n",
    "\n",
    "def get_events(case):\n",
    "    c_events = [{\n",
    "        'Case Number': case['ST_CASE'],\n",
    "# In a traffic accident AOI is Area of Impact. The spot the two cars collided is measured \n",
    "# to a fixed object, usually the curb, so it can be reconstructed later.\n",
    "        'Area of Impact': e['AOI1NAME'],\n",
    "# standard of evidence\n",
    "# https://safety.fhwa.dot.gov/rsdp/cdip_rpti.aspx\n",
    "        'Standard of Evenidence': e['SOENAME'],\n",
    "        'Event Number': e['EVENTNUM'],\n",
    "        'Vehicle 1': e['VNUMBER1'],\n",
    "        'Vehicle 2': e['VNUMBER2'],\n",
    "    } for e in case['CEvents']]\n",
    "    \n",
    "    return pd.DataFrame(c_events)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Census Search to retrieve data on all zip codes (2013 ACS5 Census)\n",
    "# See: https://github.com/CommerceDataService/census-wrapper for library documentation\n",
    "# See: https://gist.github.com/afhaque/60558290d6efd892351c4b64e5c01e9b for labels\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from census import Census\n",
    "\n",
    "# Census API Key\n",
    "from api_config import census_api_key\n",
    "c = Census(census_api_key, year=2013)\n",
    "\n",
    "\n",
    "class CensusData(object):\n",
    "    \n",
    "    census_cache = {}\n",
    "\n",
    "    @classmethod\n",
    "    def census_by_year(cls, year):\n",
    "\n",
    "        if year in cls.census_cache:\n",
    "            return cls.census_cache[year]\n",
    "\n",
    "        file_path = f'{ interim_data_dir }census_{ year }'\n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_csv(file_path)\n",
    "            if 'Year' not in df.columns:\n",
    "                df['Year'] = year\n",
    "                df.to_csv(file_path)\n",
    "            return df\n",
    "\n",
    "        try:\n",
    "            census_data = c.acs5.get((\"NAME\", \"B19013_001E\", \"B01003_001E\", \"B01002_001E\",\n",
    "                                  \"B19301_001E\",\n",
    "                                  \"B17001_002E\"), {'for': 'zip code tabulation area:*'}, year=year)\n",
    "        # Convert to DataFrame\n",
    "            census_pd = pd.DataFrame(census_data)\n",
    "\n",
    "        # Column Reordering\n",
    "            census_pd = census_pd.rename(columns={\"B01003_001E\": \"Population\",\n",
    "                                                  \"B01002_001E\": \"Median Age\",\n",
    "                                                  \"B19013_001E\": \"Household Income\",\n",
    "                                                  \"B19301_001E\": \"Per Capita Income\",\n",
    "                                                  \"B17001_002E\": \"Poverty Count\",\n",
    "                                                  \"NAME\": \"Name\",\n",
    "                                                  \"zip code tabulation area\": \"Zipcode\"})\n",
    "            census_pd['Year'] = str(year)\n",
    "            \n",
    "            census_pd.to_csv(file_path)\n",
    "            return census_pd\n",
    "\n",
    "        except:\n",
    "            print('no data')\n",
    "            return None\n",
    "        \n",
    "    @classmethod\n",
    "    def all_years(cls):\n",
    "        # load all census tables 2011 - 2018\n",
    "        years = range(2011, 2019)\n",
    "        all_years = [cls.census_by_year(y) for y in years]\n",
    "        df = pd.concat(all_years, ignore_index=True)\n",
    "        df['Zipcode'] = df['Zipcode'].astype('str')\n",
    "        df['Year'] = df['Year'].astype('str')\n",
    "        return df\n",
    "    \n",
    "    @classmethod\n",
    "    def ensure_census_columns(cls, df):\n",
    "        if 'Per Capita Income' in df.columns: return df\n",
    "        merged = pd.merge(df, cls.all_years(), how='left', left_on=['Year', 'Accident ZIP'], right_on=['Year', 'Zipcode'])\n",
    "        return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0             int64\n",
       "Unnamed: 0.1           int64\n",
       "Median Age           float64\n",
       "Population           float64\n",
       "Poverty Count        float64\n",
       "Household Income     float64\n",
       "Per Capita Income    float64\n",
       "Name                  object\n",
       "Zipcode               object\n",
       "Year                  object\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CensusData.all_years().dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from shapely.geometry import shape, Point\n",
    "# depending on your version, use: from shapely.geometry import shape, Point\n",
    "\n",
    "\n",
    "class ZipCoder(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.js = None\n",
    "    \n",
    "    def __get_zip(self, lat, lng):\n",
    "        point = Point(lng, lat)\n",
    "\n",
    "        for feature in self.js['features']:\n",
    "            polygon = shape(feature['geometry'])\n",
    "            if polygon.contains(point):\n",
    "                zip_code = feature['properties']['zip']\n",
    "                return zip_code\n",
    "\n",
    "\n",
    "    def __row_to_zip(self, r):\n",
    "        lat = float(r['Lat'])\n",
    "        lng = float(r['Lng'])\n",
    "        return self.__get_zip(lat, lng)\n",
    "\n",
    "    \n",
    "    def ensure_acc_zips(self, df):\n",
    "        with open(f'{ raw_data_dir }Zip Codes.geojson') as f:\n",
    "            self.js = json.load(f)\n",
    "            \n",
    "        acc_zip_col = 'Accident ZIP'\n",
    "\n",
    "        if acc_zip_col not in df.columns: \n",
    "            zip_codes = df.apply(self.__row_to_zip, axis=1)\n",
    "            df[acc_zip_col] = zip_codes\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mike\\Anaconda3\\lib\\site-packages\\grequests.py:21: MonkeyPatchWarning: Monkey-patching ssl after ssl has already been imported may lead to errors, including RecursionError on Python 3.6. It may also silently lead to incorrect behaviour on Python 3.7. Please monkey-patch earlier. See https://github.com/gevent/gevent/issues/1016. Modules that had direct imports (NOT patched): ['urllib3.contrib.pyopenssl (C:\\\\Users\\\\Mike\\\\Anaconda3\\\\lib\\\\site-packages\\\\urllib3\\\\contrib\\\\pyopenssl.py)', 'urllib3.util (C:\\\\Users\\\\Mike\\\\Anaconda3\\\\lib\\\\site-packages\\\\urllib3\\\\util\\\\__init__.py)']. \n",
      "  curious_george.patch_all(thread=False, select=False)\n",
      "C:\\Users\\Mike\\Anaconda3\\lib\\site-packages\\gevent\\hub.py:154: UserWarning: libuv only supports millisecond timer resolution; all times less will be set to 1 ms\n",
      "  with loop.timer(seconds, ref=ref) as t:\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import grequests\n",
    "from itertools import islice\n",
    "import os\n",
    "\n",
    "\n",
    "def chunk(it, size):\n",
    "    it = iter(it)\n",
    "    return iter(lambda: tuple(islice(it, size)), ())\n",
    "\n",
    "# LARGE CHUNK SIZE WILL BLOW UP SERVER AND CAUSING: AttributeError: 'NoneType' object has no attribute 'json'\n",
    "chunk_size = 5\n",
    "fromCaseYear = \"2010\"\n",
    "toCaseYear = \"2020\"\n",
    "state = \"6\"\n",
    "case_file_base = raw_data_dir\n",
    "\n",
    "data_lists = {}\n",
    "\n",
    "\n",
    "def url_from_row(r):\n",
    "    statecase = r[\"ST_CASE\"]\n",
    "    caseyear = r[\"CaseYear\"]\n",
    "    return f\"{url}/crashes/GetCaseDetails?stateCase={statecase}&caseYear={caseyear}&state=6&format=json\"\n",
    "\n",
    "\n",
    "def get_file_path(case):\n",
    "    return f'{ case_file_base }{ case[\"ST_CASE\"] }.json'\n",
    "    \n",
    "    \n",
    "def load_case(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        case = json.load(f)\n",
    "        return case\n",
    "\n",
    "    \n",
    "def __get_cases():\n",
    "    urls = []\n",
    "    found_locally = 0\n",
    "    for i, r in cali_df.iterrows():\n",
    "        file_path = get_file_path(r)\n",
    "        if os.path.exists(file_path):\n",
    "            found_locally += 1\n",
    "            clear_output(wait=True)\n",
    "            print(f'{ found_locally } files found locally')\n",
    "            yield load_case(file_path)\n",
    "        else:\n",
    "            url = url_from_row(r)\n",
    "            urls.append(url)\n",
    "    print(f'{ len(urls) } need to be fetched. ')\n",
    "    for c in __chunk_and_fetch(urls):\n",
    "        yield c\n",
    "    \n",
    "\n",
    "def __fetch_cases(urls):\n",
    "    rs = (grequests.get(u) for u in urls)\n",
    "    case_data = grequests.map(rs)\n",
    "    return [data.json()['Results'][0][0]['CrashResultSet'] for data in case_data]\n",
    "    \n",
    "    \n",
    "def __save_case(case):\n",
    "    file_path = get_file_path(case)\n",
    "    with open(file_path, 'w') as json_file:\n",
    "        json.dump(case, json_file)\n",
    "    \n",
    "    \n",
    "def __chunk_and_fetch(urls):\n",
    "    chunked = chunk(urls, chunk_size)\n",
    "    \n",
    "    i = 0\n",
    "    for chunked_urls in chunked:\n",
    "        i += 1\n",
    "        clear_output(wait=True)\n",
    "        print(f'Retrieving chunk { i } of { len(urls) / chunk_size } ...')\n",
    "        cases = __fetch_cases(chunked_urls) \n",
    "        for case in cases:\n",
    "            __save_case(case)\n",
    "            yield case\n",
    "\n",
    "        \n",
    "people_key = 'people'\n",
    "events_key = 'events'\n",
    "\n",
    "\n",
    "def __get_case_lists():\n",
    "    # actualize list to avoid redundant api calls\n",
    "    case_list = list(__get_cases())\n",
    "    \n",
    "    file_path_people = f\"{ interim_data_dir }people.csv\"\n",
    "    people_list = [get_people(case) for case in case_list]\n",
    "    people_df = pd.concat(people_list, ignore_index=True, sort=False)\n",
    "    people_df.to_csv(file_path_people)\n",
    "    data_lists[people_key] = people_df\n",
    "    \n",
    "    file_path_events = f\"{ interim_data_dir }events.csv\"\n",
    "    event_list = [get_events(case) for case in case_list]\n",
    "    event_df = pd.concat(event_list, sort=False)\n",
    "    event_df.to_csv(file_path_events)\n",
    "    data_lists[events_key] = event_df\n",
    "\n",
    "    return people_df, event_df\n",
    "    \n",
    "\n",
    "def __ensure_updates(df):\n",
    "    ZipCoder().ensure_acc_zips(df)\n",
    "    merged = CensusData.ensure_census_columns(df)\n",
    "    \n",
    "    unnecessary_columns = [\n",
    "        'Unnamed: 0', 'Unnamed: 0.1',\n",
    "        'Unnamed: 0_x', 'Unnamed: 0_x', \n",
    "        'Unnamed: 0.1_x', 'Unnamed: 0.1.1', \n",
    "        'Unnamed: 0_y', 'Unnamed: 0.1_y']\n",
    "    for c in unnecessary_columns:\n",
    "        if c in df.columns:\n",
    "            merged.drop(c,\n",
    "                1,\n",
    "                inplace=True)\n",
    "            \n",
    "    renames = {\n",
    "        'ZIP Code': 'Driver ZIP Code',\n",
    "    }\n",
    "    \n",
    "    for k in renames.keys():\n",
    "        if k in merged.columns:\n",
    "            merged.rename(columns=renames, inplace=True)\n",
    "    \n",
    "    file_path = f\"{ interim_data_dir }people.csv\"\n",
    "    merged.to_csv(file_path)\n",
    "    \n",
    "    return merged\n",
    "    \n",
    "    \n",
    "def get_people_list():\n",
    "    cached = get_cached_list(people_key)\n",
    "    if cached is not None:\n",
    "        return __ensure_updates(cached)\n",
    "    df = __get_case_lists()[0]\n",
    "    return __ensure_updates(df)\n",
    "\n",
    "\n",
    "def get_event_list():\n",
    "    cached = get_cached_list(events_key)\n",
    "    if cached is not None:\n",
    "        return cached\n",
    "    df = __get_case_lists()[1]\n",
    "    file_path = f\"{ interim_data_dir }events.csv\"\n",
    "    df.to_csv(file_path)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_cached_list(key):\n",
    "    if key in data_lists:\n",
    "        return data_lists[key]\n",
    "    \n",
    "    file_path = f\"{ interim_data_dir }{ key }.csv\"\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        data_lists[key] = \n",
    "        \\df\n",
    "        return df\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_people_list()\n",
    "grouped = df.groupby('Case Number').count()\n",
    "assert len(cali_df) == len(grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Zipcode</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Timezone</th>\n",
       "      <th>Daylight savings time flag</th>\n",
       "      <th>geopoint</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>71937</td>\n",
       "      <td>Cove</td>\n",
       "      <td>AR</td>\n",
       "      <td>34.398483</td>\n",
       "      <td>-94.39398</td>\n",
       "      <td>-6</td>\n",
       "      <td>1</td>\n",
       "      <td>34.398483</td>\n",
       "      <td>-94.39398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>72044</td>\n",
       "      <td>Edgemont</td>\n",
       "      <td>AR</td>\n",
       "      <td>35.624351</td>\n",
       "      <td>-92.16056</td>\n",
       "      <td>-6</td>\n",
       "      <td>1</td>\n",
       "      <td>35.624351</td>\n",
       "      <td>-92.16056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56171</td>\n",
       "      <td>Sherburn</td>\n",
       "      <td>MN</td>\n",
       "      <td>43.660847</td>\n",
       "      <td>-94.74357</td>\n",
       "      <td>-6</td>\n",
       "      <td>1</td>\n",
       "      <td>43.660847</td>\n",
       "      <td>-94.74357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49430</td>\n",
       "      <td>Lamont</td>\n",
       "      <td>MI</td>\n",
       "      <td>43.010337</td>\n",
       "      <td>-85.89754</td>\n",
       "      <td>-5</td>\n",
       "      <td>1</td>\n",
       "      <td>43.010337</td>\n",
       "      <td>-85.89754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52585</td>\n",
       "      <td>Richland</td>\n",
       "      <td>IA</td>\n",
       "      <td>41.194129</td>\n",
       "      <td>-91.98027</td>\n",
       "      <td>-6</td>\n",
       "      <td>1</td>\n",
       "      <td>41.194129</td>\n",
       "      <td>-91.98027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Zipcode      City State   Latitude  Longitude  Timezone  \\\n",
       "0    71937      Cove    AR  34.398483  -94.39398        -6   \n",
       "1    72044  Edgemont    AR  35.624351  -92.16056        -6   \n",
       "2    56171  Sherburn    MN  43.660847  -94.74357        -6   \n",
       "3    49430    Lamont    MI  43.010337  -85.89754        -5   \n",
       "4    52585  Richland    IA  41.194129  -91.98027        -6   \n",
       "\n",
       "   Daylight savings time flag   geopoint  Unnamed: 8  \n",
       "0                           1  34.398483   -94.39398  \n",
       "1                           1  35.624351   -92.16056  \n",
       "2                           1  43.660847   -94.74357  \n",
       "3                           1  43.010337   -85.89754  \n",
       "4                           1  41.194129   -91.98027  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import downloaded data from https://public.opendatasoft.com/explore/dataset/us-zip-code-latitude-and-longitude/table/.\n",
    "# Use dtype=\"object\" to match other\n",
    "#zip_latlng = pd.read_csv(\"zip_latlng.csv\", dtype=\"object\")\n",
    "zip_latlng = pd.read_csv(\"../Data/interim/zip_latlng.csv\")\n",
    "\n",
    "zip_latlng = zip_latlng.rename(columns={\"Zip\": \"Zipcode\"})\n",
    "\n",
    "# Visualize\n",
    "zip_latlng.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       92108.0\n",
       "1       92108.0\n",
       "2       92028.0\n",
       "3       92154.0\n",
       "4       92154.0\n",
       "5       92019.0\n",
       "6       92019.0\n",
       "7       92019.0\n",
       "8       92107.0\n",
       "9       92107.0\n",
       "10      92107.0\n",
       "11      92107.0\n",
       "12      92104.0\n",
       "13      92029.0\n",
       "14      91941.0\n",
       "15      91950.0\n",
       "16      91950.0\n",
       "17      92065.0\n",
       "18      92065.0\n",
       "19      92065.0\n",
       "20      92071.0\n",
       "21      92071.0\n",
       "22      91911.0\n",
       "23      91911.0\n",
       "24      91911.0\n",
       "25      91911.0\n",
       "26      91911.0\n",
       "27      91911.0\n",
       "28      91911.0\n",
       "29      91911.0\n",
       "         ...   \n",
       "3996    91941.0\n",
       "3997    91941.0\n",
       "3998    91941.0\n",
       "3999    91941.0\n",
       "4000    91941.0\n",
       "4001    91941.0\n",
       "4002    91941.0\n",
       "4003    92020.0\n",
       "4004    92021.0\n",
       "4005    92021.0\n",
       "4006    92021.0\n",
       "4007    92021.0\n",
       "4008    92021.0\n",
       "4009    92021.0\n",
       "4010    92126.0\n",
       "4011    91911.0\n",
       "4012    91911.0\n",
       "4013    91911.0\n",
       "4014    91911.0\n",
       "4015    92084.0\n",
       "4016    91911.0\n",
       "4017    91911.0\n",
       "4018    91911.0\n",
       "4019    91911.0\n",
       "4020    91911.0\n",
       "4021    92025.0\n",
       "4022    92025.0\n",
       "4023    91945.0\n",
       "4024    91910.0\n",
       "4025    92101.0\n",
       "Name: Accident ZIP, Length: 4026, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_people_list()['Accident ZIP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31675\n"
     ]
    }
   ],
   "source": [
    "# Merge the two data sets along zip code\n",
    "data_complete = pd.merge(\n",
    "    zip_latlng, census_pd, how=\"left\", on=[\"Zipcode\", \"Zipcode\"])\n",
    "\n",
    "# Remove rows missing data\n",
    "data_complete = data_complete.dropna()\n",
    "print(len(data_complete))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Zipcode</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Timezone</th>\n",
       "      <th>Daylight savings time flag</th>\n",
       "      <th>geopoint</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Population</th>\n",
       "      <th>Poverty Count</th>\n",
       "      <th>Household Income</th>\n",
       "      <th>Per Capita Income</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>92114</td>\n",
       "      <td>San Diego</td>\n",
       "      <td>CA</td>\n",
       "      <td>32.706954</td>\n",
       "      <td>-117.05420</td>\n",
       "      <td>-8</td>\n",
       "      <td>1</td>\n",
       "      <td>32.706954</td>\n",
       "      <td>-117.05420</td>\n",
       "      <td>28505.0</td>\n",
       "      <td>33.2</td>\n",
       "      <td>69798.0</td>\n",
       "      <td>11762.0</td>\n",
       "      <td>56679.0</td>\n",
       "      <td>18915.0</td>\n",
       "      <td>ZCTA5 92114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3115</th>\n",
       "      <td>92108</td>\n",
       "      <td>San Diego</td>\n",
       "      <td>CA</td>\n",
       "      <td>32.773600</td>\n",
       "      <td>-117.13785</td>\n",
       "      <td>-8</td>\n",
       "      <td>1</td>\n",
       "      <td>32.773600</td>\n",
       "      <td>-117.13785</td>\n",
       "      <td>28500.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>20548.0</td>\n",
       "      <td>3080.0</td>\n",
       "      <td>65398.0</td>\n",
       "      <td>40431.0</td>\n",
       "      <td>ZCTA5 92108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3881</th>\n",
       "      <td>92139</td>\n",
       "      <td>San Diego</td>\n",
       "      <td>CA</td>\n",
       "      <td>32.677286</td>\n",
       "      <td>-117.05241</td>\n",
       "      <td>-8</td>\n",
       "      <td>1</td>\n",
       "      <td>32.677286</td>\n",
       "      <td>-117.05241</td>\n",
       "      <td>28525.0</td>\n",
       "      <td>33.1</td>\n",
       "      <td>36934.0</td>\n",
       "      <td>4956.0</td>\n",
       "      <td>58050.0</td>\n",
       "      <td>20371.0</td>\n",
       "      <td>ZCTA5 92139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4169</th>\n",
       "      <td>92102</td>\n",
       "      <td>San Diego</td>\n",
       "      <td>CA</td>\n",
       "      <td>32.714992</td>\n",
       "      <td>-117.12537</td>\n",
       "      <td>-8</td>\n",
       "      <td>1</td>\n",
       "      <td>32.714992</td>\n",
       "      <td>-117.12537</td>\n",
       "      <td>28494.0</td>\n",
       "      <td>30.7</td>\n",
       "      <td>45461.0</td>\n",
       "      <td>13290.0</td>\n",
       "      <td>41840.0</td>\n",
       "      <td>19543.0</td>\n",
       "      <td>ZCTA5 92102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6499</th>\n",
       "      <td>92118</td>\n",
       "      <td>Coronado</td>\n",
       "      <td>CA</td>\n",
       "      <td>32.682727</td>\n",
       "      <td>-117.17441</td>\n",
       "      <td>-8</td>\n",
       "      <td>1</td>\n",
       "      <td>32.682727</td>\n",
       "      <td>-117.17441</td>\n",
       "      <td>28509.0</td>\n",
       "      <td>39.8</td>\n",
       "      <td>22939.0</td>\n",
       "      <td>1207.0</td>\n",
       "      <td>90256.0</td>\n",
       "      <td>50693.0</td>\n",
       "      <td>ZCTA5 92118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Zipcode       City State   Latitude  Longitude  Timezone  \\\n",
       "415     92114  San Diego    CA  32.706954 -117.05420        -8   \n",
       "3115    92108  San Diego    CA  32.773600 -117.13785        -8   \n",
       "3881    92139  San Diego    CA  32.677286 -117.05241        -8   \n",
       "4169    92102  San Diego    CA  32.714992 -117.12537        -8   \n",
       "6499    92118   Coronado    CA  32.682727 -117.17441        -8   \n",
       "\n",
       "      Daylight savings time flag   geopoint  Unnamed: 8  Unnamed: 0  \\\n",
       "415                            1  32.706954  -117.05420     28505.0   \n",
       "3115                           1  32.773600  -117.13785     28500.0   \n",
       "3881                           1  32.677286  -117.05241     28525.0   \n",
       "4169                           1  32.714992  -117.12537     28494.0   \n",
       "6499                           1  32.682727  -117.17441     28509.0   \n",
       "\n",
       "      Median Age  Population  Poverty Count  Household Income  \\\n",
       "415         33.2     69798.0        11762.0           56679.0   \n",
       "3115        32.0     20548.0         3080.0           65398.0   \n",
       "3881        33.1     36934.0         4956.0           58050.0   \n",
       "4169        30.7     45461.0        13290.0           41840.0   \n",
       "6499        39.8     22939.0         1207.0           90256.0   \n",
       "\n",
       "      Per Capita Income         Name  \n",
       "415             18915.0  ZCTA5 92114  \n",
       "3115            40431.0  ZCTA5 92108  \n",
       "3881            20371.0  ZCTA5 92139  \n",
       "4169            19543.0  ZCTA5 92102  \n",
       "6499            50693.0  ZCTA5 92118  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd_cty_zip = [\n",
    "92152,\n",
    "92196,\n",
    "92192,\n",
    "92161,\n",
    "92132,\n",
    "92193,\n",
    "92143,\n",
    "92138,\n",
    "92182,\n",
    "92198,\n",
    "92150,\n",
    "92199,\n",
    "92038,\n",
    "92140,\n",
    "92112,\n",
    "92093,\n",
    "92145,\n",
    "92092,\n",
    "92091,\n",
    "92014,\n",
    "92173,\n",
    "92027,\n",
    "92118,\n",
    "92119,\n",
    "92124,\n",
    "92106,\n",
    "92107,\n",
    "92116,\n",
    "92139,\n",
    "92029,\n",
    "92071,\n",
    "92113,\n",
    "92102,\n",
    "92104,\n",
    "92025,\n",
    "92037,\n",
    "92120,\n",
    "92110,\n",
    "91945,\n",
    "92129,\n",
    "92105,\n",
    "92103,\n",
    "92131,\n",
    "92114,\n",
    "92117,\n",
    "91942,\n",
    "92128,\n",
    "92111,\n",
    "92122,\n",
    "91932,\n",
    "92109,\n",
    "92126,\n",
    "92127,\n",
    "92123,\n",
    "92108,\n",
    "92115,\n",
    "92121,\n",
    "92154,\n",
    "92130,\n",
    "92101,\n",
    "91911]\n",
    "\n",
    "data_complete_dropneg = data_complete.loc[(data_complete[\"Per Capita Income\"] > 0) &\n",
    "                                          (data_complete[\"Zipcode\"].isin(sd_cty_zip)),\n",
    "                                          :]\n",
    "\n",
    "# Visualize\n",
    "print(len(data_complete_dropneg))\n",
    "data_complete_dropneg.head()\n",
    "\n",
    "#data_complete.to_csv(\"data_complete.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'g_key' from 'api_config' (C:\\Users\\Mike\\Google Drive\\ucsd_extension\\assignments\\ucsd_bootcamp-traffic_accident_analysis\\notebooks\\api_config.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-2db60633c529>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Import API key\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mapi_config\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mg_key\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Configure gmaps with API key\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'g_key' from 'api_config' (C:\\Users\\Mike\\Google Drive\\ucsd_extension\\assignments\\ucsd_bootcamp-traffic_accident_analysis\\notebooks\\api_config.py)"
     ]
    }
   ],
   "source": [
    "import gmaps\n",
    "\n",
    "# Import API key\n",
    "from api_config import g_key\n",
    "\n",
    "# Configure gmaps with API key\n",
    "gmaps.configure(api_key=g_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store 'Lat' and 'Lng' into  locations \n",
    "locations = data_complete_dropneg[[\"Latitude\", \"Longitude\"]].astype(float)\n",
    "\n",
    "# Convert income and age to float and store\n",
    "# HINT: be sure to handle NaN values\n",
    "income = data_complete_dropneg[\"Per Capita Income\"].astype(float)\n",
    "#age = data_complete_dropneg[\"Median Age\"].astype(float)\n",
    "#data_complete_dropneg[\"Per Capita Income\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an income Heatmap layer\n",
    "fig = gmaps.figure()\n",
    "\n",
    "heat_layer = gmaps.heatmap_layer(locations, weights=income, \n",
    "                                 dissipating=True,\n",
    "                                 point_radius = 10)\n",
    "\n",
    "fig.add_layer(heat_layer)\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_symbol_layer = gmaps.symbol_layer(\n",
    "    locations, fill_color='blue',\n",
    "    stroke_color='blue', scale=3,\n",
    "#    locations_accident, fill_color='rgba(0, 150, 0, 0.4)',\n",
    "#    stroke_color='rgba(0, 0, 150, 0.4)', scale=3,\n",
    "#    info_box_content=[f\"Bank amount: {bank}\" for bank in bank_rate]\n",
    ")\n",
    "\n",
    "fig = gmaps.figure()\n",
    "fig.add_layer(income_symbol_layer)\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_df_new = pd.read_csv(\"people.csv\")\n",
    "people_df_new.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_df_new['Case Count'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_df_new2 = people_df_new.groupby(['Case Number'], as_index=False).agg({'Case Count': 'count', 'Lat': 'first', 'Lng': 'first'})\n",
    "people_df_new2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert accident frequency to list\n",
    "top_accidents = people_df_new2.nlargest(10, \"Case Count\")\n",
    "top_accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_accident = top_accidents[[\"Lat\", \"Lng\"]].astype(float)\n",
    "\n",
    "accident_rate = top_accidents['Case Count'].tolist()\n",
    "#name = hotel_df['Hotel Name'].tolist()\n",
    "#city = hotel_df['City'].tolist()\n",
    "#country = hotel_df['Country'].tolist()\n",
    "\n",
    "#fig = gmaps.figure(layout=figure_layout)\n",
    "fig = gmaps.figure()\n",
    "\n",
    "# Assign the marker layer to a variable\n",
    "#markers = gmaps.marker_layer(locations_accident, info_box_content=hotel_info)\n",
    "markers = gmaps.marker_layer(locations_accident)\n",
    "\n",
    "# Add the layer to the map\n",
    "fig.add_layer(markers)\n",
    "\n",
    "# Display Map\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create accident layer\n",
    "accident_layer = gmaps.symbol_layer(\n",
    "    locations_accident, fill_color='red',\n",
    "    stroke_color='red', scale=2,\n",
    "#    locations_accident, fill_color='rgba(0, 150, 0, 0.4)',\n",
    "#    stroke_color='rgba(0, 0, 150, 0.4)', scale=3,\n",
    "#    info_box_content=[f\"Bank amount: {bank}\" for bank in bank_rate]\n",
    ")\n",
    "\n",
    "\n",
    "fig = gmaps.figure()\n",
    "fig.add_layer(accident_layer)\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an accident Heatmap layer\n",
    "fig = gmaps.figure()\n",
    "\n",
    "accident_heat_layer = gmaps.heatmap_layer(locations_accident, weights=accident_rate, \n",
    "                                          dissipating=True,\n",
    "                                          point_radius = 10, \n",
    "                                         )\n",
    "\n",
    "\n",
    "fig.add_layer(accident_heat_layer)\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a combined map\n",
    "fig = gmaps.figure()\n",
    "\n",
    "fig.add_layer(heat_layer)            # weighted by income\n",
    "#fig.add_layer(markers)              # top number of accidents\n",
    "fig.add_layer(accident_layer)        # top number of accidents\n",
    "#fig.add_layer(income_symbol_layer)\n",
    "\n",
    "fig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "nbdime-conflicts": {
   "local_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "version",
       "op": "patch"
      }
     ],
     "key": "language_info",
     "op": "patch"
    }
   ],
   "remote_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "diff": [
          {
           "key": 2,
           "op": "addrange",
           "valuelist": "7"
          },
          {
           "key": 2,
           "length": 1,
           "op": "removerange"
          },
          {
           "key": 5,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": 0,
         "op": "patch"
        }
       ],
       "key": "version",
       "op": "patch"
      }
     ],
     "key": "language_info",
     "op": "patch"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
